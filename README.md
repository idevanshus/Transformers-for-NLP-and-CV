# Transformers-for-NLP-and-CV
Welcome to the learning repository for the book "Transformers for Natural Language Processing and Computer Vision, Third Edition" by Denis Rothman. This repository contains my journey, notes, and code implementations as I learn and apply concepts from this insightful book.

## About the Book

This book provides an in-depth exploration of transformer architectures and their applications in Natural Language Processing (NLP) and Computer Vision (CV). It covers essential transformer models, including BERT, GPT, Vision Transformers (ViT), and more. The book also delves into practical techniques like fine-tuning, pretraining, and real-world applications.

This repository tracks my progress as I implement concepts from *Transformers for Natural Language Processing and Computer Vision, Third Edition* by Denis Rothman. Each section focuses on applied topics, and I've included small notes about my learning journey.

## Progress Tracker

---

### **1. Core Transformer Concepts**
| **Topic**                          | **Status** | **Notebook Link** | **Notes** |
|------------------------------------|------------|-------------------|-----------|
| Transformer Architecture           |    ✅      |                   | Learned how self-attention works and why it’s so powerful. Struggled with positional encoding at first, but it clicked after visualizing it. |
| Hugging Face Models                |      ✅       |                   | Explored pre-trained models like BERT and GPT. Hugging Face’s API is a game-changer for quick prototyping. |
| Attention Mechanisms               |         ✅    |                   | Implemented multi-head attention from scratch. It’s fascinating how it captures contextual relationships. |

---

### **2. NLP Applications**
| **Topic**                          | **Status** | **Notebook Link** | **Notes** |
|------------------------------------|------------|-------------------|-----------|
| Fine-Tuning BERT                   |            |                   | Fine-tuned BERT on a sentiment analysis task. Took some time to understand the data preprocessing pipeline. |
| Machine Translation (Google Trax)  |            |                   | Built a translation model using Google Trax. The hardest part was preprocessing the WMT dataset. |
| Text Summarization with T5         |            |                   | Used T5 for summarization. It’s amazing how well it works out of the box. |
| Semantic Role Labeling with GPT-4  |            |                   | Experimented with GPT-4 for SRL. It’s surprisingly good at understanding complex sentences. |

---

### **3. Generative AI**
| **Topic**                          | **Status** | **Notebook Link** | **Notes** |
|------------------------------------|------------|-------------------|-----------|
| ChatGPT and GPT-4 Applications     |            |                   | Built a chatbot using GPT-4. The API is super easy to use, but prompt engineering is an art! |
| Fine-Tuning GPT Models             |            |                   | Fine-tuned GPT-3 for a custom text generation task. Learned the importance of dataset quality. |
| Retrieval Augmented Generation     |            |                   | Combined GPT-4 with a knowledge base for RAG. It’s like giving the model a memory! |

---

## **4. Computer Vision with Transformers**
| **Topic**                          | **Status** | **Notebook Link** | **Notes** |
|------------------------------------|------------|-------------------|-----------|
| Vision Transformers (ViT)          |            |                   | Implemented ViT for image classification. It’s incredible how transformers work for both text and images. |
| CLIP and DALL-E Models             |            |                   | Played with CLIP for zero-shot image classification. DALL-E’s creativity is mind-blowing. |
| Stable Diffusion for Image Generation |            |                   | Generated images using Stable Diffusion. The results are stunning, but it’s computationally expensive. |

---

## **5. Advanced Topics**
| **Topic**                          | **Status** | **Notebook Link** | **Notes** |
|------------------------------------|------------|-------------------|-----------|
| Tokenizers and Embeddings          |            |                   | Explored WordPiece and Byte-Pair Encoding. Tokenization is more nuanced than I thought. |
| Risk Mitigation in LLMs            |            |                   | Learned about RLHF and RAG for reducing hallucinations in LLMs. Ethical AI is crucial! |
| Functional AGI with HuggingGPT     |            |                   | Chained multiple models using HuggingGPT. It’s like building a pipeline of AI agents. |

---

## **6. Tools and Libraries**
| **Topic**                          | **Status** | **Notebook Link** | **Notes** |
|------------------------------------|------------|-------------------|-----------|
| Hugging Face AutoTrain             |            |                   | Used AutoTrain to fine-tune a model without writing code. Great for quick experiments. |
| Vertex AI and PaLM 2               |            |                   | Explored PaLM 2 on Vertex AI. The integration with Google Cloud is seamless. |
| Interpretability with BertViz      |            |                   | Visualized attention heads with BertViz. It’s like peeking into the model’s brain. |

---

## Contributions

This is a personal learning repository and contributions are not expected. However, if you spot errors or have suggestions, feel free to open an issue or submit a pull request.

## Acknowledgments

A special thanks to Denis Rothman for his comprehensive book on transformers, and to the open-source community for providing exceptional tools and resources.
