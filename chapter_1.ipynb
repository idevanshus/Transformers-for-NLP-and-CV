{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computational Complexity in NLP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational times of complexity per layer \n",
    "# Comparing the computational time between: \n",
    "# self attention = O(n^2 * d)\n",
    "# recurrent = O(n * d^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sequence Length and Dimensionality.\n",
    "# Set the sequence length `n` and the representation dimensionality `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 512\n",
    "d = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Random Input Sequence\n",
    "# Create a random input sequence of shape `(n, d)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = np.random.rand(n, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation of self-attention layer O(n^2*d)\n",
    "start_time = time.time()\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        _ = np.dot(input_seq[i], input_seq[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention computation time: 0.2576613426208496 seconds\n"
     ]
    }
   ],
   "source": [
    "at = time.time() - start_time\n",
    "print(f\"Self-attention computation time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recurrent layer computation time: 36.8253710269928 seconds\n"
     ]
    }
   ],
   "source": [
    "# simulation of recurrent layer O(n*d^2)\n",
    "start_time = time.time()\n",
    "hidden_state = np.zeros(d)\n",
    "for i in range (n):\n",
    "    for j in range(d):\n",
    "        for k in range(d):\n",
    "            hidden_state[j] += input_seq[i, j] * hidden_state[k]\n",
    "rt = time.time() - start_time\n",
    "print(f\"Recurrent layer computation time: {time.time() - start_time} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of 'computational time for attention' in the sum of 'attention' and 'recurrent' is 0.69%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total\n",
    "total = at + rt\n",
    "\n",
    "# Calculate the percentage of at\n",
    "percentage_at = round((at / total) * 100,2)\n",
    "\n",
    "# Output the result\n",
    "print(f\"The percentage of 'computational time for attention' in the sum of 'attention' and 'recurrent' is {percentage_at}%\")\n",
    "\n",
    "# Calculate x, which is the ratio of rt to at\n",
    "x = round(rt / at,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "# Install torch package\n",
    "%pip install torch\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "  print(\"No GPU available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Self-attention computation time: 0.0007717609405517578 seconds\n",
      "Recurrent layer computation time: 2.078526020050049 seconds\n",
      "The percentage of self-attention computation in the sum of self-attention and recurrent computation is 0.04%\n"
     ]
    }
   ],
   "source": [
    "# PyTorch version\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# define the sequence length and representation dimensionality\n",
    "n = 512\n",
    "d = 512\n",
    "\n",
    "# Use GPU if available, otherwise stick with cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# define the inputs\n",
    "input_seq = torch.rand(n, d, device=device)\n",
    "\n",
    "# simulation of self-attention layer O(n^2*d)\n",
    "start_time = time.time()\n",
    "_ = torch.mm(input_seq, input_seq.t())\n",
    "at = time.time() - start_time\n",
    "print(f\"Self-attention computation time: {at} seconds\")\n",
    "\n",
    "# simulation of recurrent layer O(n*d^2)\n",
    "start_time = time.time()\n",
    "hidden_state = torch.zeros(d, device=device)\n",
    "for i in range(n):\n",
    "    for j in range(d):\n",
    "        for k in range(d):\n",
    "            hidden_state[j] += input_seq[i, j] * hidden_state[k]\n",
    "            ct = time.time() - start_time\n",
    "            if ct>at*10:\n",
    "              break\n",
    "\n",
    "rt = time.time() - start_time\n",
    "print(f\"Recurrent layer computation time: {rt} seconds\")\n",
    "\n",
    "# Calculate the total\n",
    "total = at + rt\n",
    "\n",
    "# Calculate the percentage of at\n",
    "percentage_at = round((at / total) * 100, 2)\n",
    "\n",
    "# Output the result\n",
    "print(f\"The percentage of self-attention computation in the sum of self-attention and recurrent computation is {percentage_at}%\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Implementation.\n",
    "# Using TensorFlow to simulate the self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention computation time: 52.23362469673157 seconds\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# define the sequence length and representation dimensionality\n",
    "n =  32768\n",
    "d = 12288\n",
    "\n",
    "# define the inputs\n",
    "input_seq = tf.random.normal((n, d), dtype=tf.float32)\n",
    "\n",
    "# simulation of self-attention layer O(n^2*d)\n",
    "start_time = time.time()\n",
    "_ = tf.matmul(input_seq, input_seq, transpose_b=True)\n",
    "\n",
    "at = time.time() - start_time\n",
    "print(f\"Self-attention computation time: {at} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Architecture\n",
    "# Implementation of the Transformer architecture step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x =np.array([[1.0, 0.0, 1.0, 0.0],   # Input 1\n",
    "             [0.0, 2.0, 0.0, 2.0],   # Input 2\n",
    "             [1.0, 1.0, 1.0, 1.0]])  # Input 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_model=4\n",
      "w_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
    "print(\"w_query\")\n",
    "w_query =np.array([[1, 0, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 1]])\n",
    "print(w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 1, 0]])\n",
    "print(w_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"w_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "                    [0, 3, 0],\n",
    "                    [1, 0, 3],\n",
    "                    [1, 1, 0]])\n",
    "print(w_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Query: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "print(\"Query: x * w_query\")\n",
    "Q=np.matmul(x,w_query)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Key: x * w_key\")\n",
    "K=np.matmul(x,w_key)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Value: x * w_value\")\n",
    "V=np.matmul(x,w_value)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Scaled Attention Scores\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Scaled Attention Scores\")\n",
    "k_d=1 #square root of k_d=3 rounded down to 1 for this example\n",
    "attention_scores = (Q @ K.transpose())/k_d\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Scaled softmax attention_scores for each vector\n",
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
    "attention_scores[0]=softmax(attention_scores[0])\n",
    "attention_scores[1]=softmax(attention_scores[1])\n",
    "attention_scores[2]=softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: attention value obtained by score1/k_d * V\n",
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n",
      "Attention 1\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "Attention 2\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "Attention 3\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: attention value obtained by score1/k_d * V\")\n",
    "print(V[0])\n",
    "print(V[1])\n",
    "print(V[2])\n",
    "print(\"Attention 1\")\n",
    "attention1=attention_scores[0].reshape(-1,1)\n",
    "attention1=attention_scores[0][0]*V[0]\n",
    "print(attention1)\n",
    "print(\"Attention 2\")\n",
    "attention2=attention_scores[0][1]*V[1]\n",
    "print(attention2)\n",
    "print(\"Attention 3\")\n",
    "attention3=attention_scores[0][2]*V[2]\n",
    "print(attention3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: summed the results to create the first line of the output matrix\n",
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 7: summed the results to create the first line of the output matrix\")\n",
    "attention_input1=attention1+attention2+attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Step 1 to 7 for inputs 1 to 3\n",
      "[[0.21962113 0.23332887 0.24878814 0.94922195 0.11339154 0.64616943\n",
      "  0.14884462 0.05633224 0.79976097 0.53969387 0.04226782 0.17803405\n",
      "  0.42711866 0.15817828 0.98084094 0.32622351 0.93611146 0.32823529\n",
      "  0.44039233 0.578684   0.36234751 0.51296216 0.39565728 0.68134302\n",
      "  0.01266175 0.4989774  0.09607441 0.67107334 0.01587283 0.96229691\n",
      "  0.82608546 0.49660398 0.72202009 0.20145437 0.124417   0.67969621\n",
      "  0.83618465 0.44481095 0.66211576 0.87787996 0.69272964 0.21599088\n",
      "  0.67345826 0.66400322 0.40880414 0.11889926 0.55761756 0.97270575\n",
      "  0.03783002 0.67044453 0.05130788 0.27384528 0.83421293 0.79341759\n",
      "  0.81594851 0.83862227 0.07235664 0.00217145 0.71107092 0.34004817\n",
      "  0.38062029 0.97075716 0.62579064 0.34505933]\n",
      " [0.05460562 0.35403144 0.16110127 0.85504687 0.08574397 0.95234895\n",
      "  0.56794388 0.11438117 0.37297142 0.35361358 0.73072716 0.13568564\n",
      "  0.53444797 0.26974749 0.71811678 0.8074681  0.20803768 0.77733578\n",
      "  0.62595064 0.86510122 0.81456149 0.32857015 0.73948588 0.14429832\n",
      "  0.05148933 0.52428949 0.83208559 0.22493312 0.28274381 0.17737783\n",
      "  0.40142021 0.90908224 0.60394711 0.43380694 0.25342005 0.71527787\n",
      "  0.3180268  0.53219208 0.91904819 0.53753189 0.3534643  0.64956923\n",
      "  0.77023271 0.36376571 0.12550082 0.05922571 0.67702021 0.95254923\n",
      "  0.71004405 0.3189762  0.26094267 0.81995921 0.25627847 0.25776666\n",
      "  0.33450837 0.11349963 0.71257758 0.93369441 0.86583889 0.04291308\n",
      "  0.3864956  0.63647484 0.33719713 0.52663549]\n",
      " [0.13030825 0.36353901 0.21630311 0.57506642 0.56932647 0.3527432\n",
      "  0.36735351 0.71155989 0.13258899 0.30991232 0.20603785 0.45824954\n",
      "  0.23168912 0.82084838 0.55234062 0.6417931  0.86256673 0.93492547\n",
      "  0.14734209 0.12107337 0.95785434 0.40691705 0.53782814 0.38214187\n",
      "  0.2679196  0.53087265 0.69559603 0.79789051 0.32963116 0.36651461\n",
      "  0.60223099 0.11773809 0.05099846 0.83442535 0.51308053 0.45129865\n",
      "  0.21425565 0.24783804 0.53121012 0.70636081 0.94934833 0.07276617\n",
      "  0.62814042 0.12571149 0.87371377 0.5154077  0.40543277 0.81018898\n",
      "  0.04200754 0.74404221 0.03282376 0.64847435 0.46356291 0.49867756\n",
      "  0.51903502 0.32037194 0.21279566 0.09640497 0.83204024 0.19397837\n",
      "  0.57041115 0.15723919 0.85684117 0.48983974]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
    "#We assume we have 3 results with learned weights (they were not trained in this example)\n",
    "#We assume we are implementing the original Transformer paper. We will have 3 results of 64 dimensions each\n",
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: We assume we have trained the 8 heads of the attention sublayer\n",
      "shape of one head (3, 64) dimension of 8 heads 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 9: We assume we have trained the 8 heads of the attention sublayer\")\n",
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 ouput dimension of the model\n",
      "[[0.45113299 0.92588276 0.13288348 ... 0.43085643 0.31020755 0.31293182]\n",
      " [0.42274075 0.78297358 0.11060556 ... 0.17981774 0.55364099 0.84168673]\n",
      " [0.59465149 0.53783302 0.70572621 ... 0.38161594 0.52783578 0.06286411]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 10: Concantenation of heads 1 to 8 to obtain the original 8x64=512 ouput dimension of the model\")\n",
    "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging Face Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /opt/anaconda3/lib/python3.12/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.2)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b215686670843f4a23aa414b4fcfec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/t5-base/a90903540cc02cbeb7ff9f823f1a80eb778c7e22426a0e620b01c77a5ec8f5b4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737439645&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzQzOTY0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby90NS1iYXNlL2E5MDkwMzU0MGNjMDJjYmViN2ZmOWY4MjNmMWE4MGViNzc4YzdlMjI0MjZhMGU2MjBiMDFjNzdhNWVjOGY1YjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Q6gotejgEgF3WVzCX1MJ69dsB7GDwlReLOHBB9hM6qMs%7EML22Hefwjv%7EiUNQ9j7KGWZsb-KmO4BJ2LaX2AVRJr1inPnAV1egTuD2knuuFNSqlB90yLcQB%7ExfVkH27HraxF9WWRpHBhPRU8QVp2Swp5FxEbsAZniEWV1TuAN4TWzqMjVloh2LF8uQA2npAq39k7pUwxt%7E0iqV1Kbu4wWrxeTUB%7ELTDpllLmY-9aZ4CGCwOZA1DIEjoVCkbk4D5JVaqQAB6wLyTzwKD%7E7bSUFHOaAH3R%7E%7EL0hX6ig%7EYdwA7ycujiPlRrMQ91n1MkwOBbO91IYkBAZqvi2Y-m4diOyAbA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c766d3fbb2f14c74bf7f4cc31a609844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  96%|#########6| 860M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/t5-base/a90903540cc02cbeb7ff9f823f1a80eb778c7e22426a0e620b01c77a5ec8f5b4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737439645&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzQzOTY0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby90NS1iYXNlL2E5MDkwMzU0MGNjMDJjYmViN2ZmOWY4MjNmMWE4MGViNzc4YzdlMjI0MjZhMGU2MjBiMDFjNzdhNWVjOGY1YjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Q6gotejgEgF3WVzCX1MJ69dsB7GDwlReLOHBB9hM6qMs%7EML22Hefwjv%7EiUNQ9j7KGWZsb-KmO4BJ2LaX2AVRJr1inPnAV1egTuD2knuuFNSqlB90yLcQB%7ExfVkH27HraxF9WWRpHBhPRU8QVp2Swp5FxEbsAZniEWV1TuAN4TWzqMjVloh2LF8uQA2npAq39k7pUwxt%7E0iqV1Kbu4wWrxeTUB%7ELTDpllLmY-9aZ4CGCwOZA1DIEjoVCkbk4D5JVaqQAB6wLyTzwKD%7E7bSUFHOaAH3R%7E%7EL0hX6ig%7EYdwA7ycujiPlRrMQ91n1MkwOBbO91IYkBAZqvi2Y-m4diOyAbA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90294e5995074ab0922f37d3a7e7fc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  98%|#########7| 870M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/t5-base/a90903540cc02cbeb7ff9f823f1a80eb778c7e22426a0e620b01c77a5ec8f5b4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737439645&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzQzOTY0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby90NS1iYXNlL2E5MDkwMzU0MGNjMDJjYmViN2ZmOWY4MjNmMWE4MGViNzc4YzdlMjI0MjZhMGU2MjBiMDFjNzdhNWVjOGY1YjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Q6gotejgEgF3WVzCX1MJ69dsB7GDwlReLOHBB9hM6qMs%7EML22Hefwjv%7EiUNQ9j7KGWZsb-KmO4BJ2LaX2AVRJr1inPnAV1egTuD2knuuFNSqlB90yLcQB%7ExfVkH27HraxF9WWRpHBhPRU8QVp2Swp5FxEbsAZniEWV1TuAN4TWzqMjVloh2LF8uQA2npAq39k7pUwxt%7E0iqV1Kbu4wWrxeTUB%7ELTDpllLmY-9aZ4CGCwOZA1DIEjoVCkbk4D5JVaqQAB6wLyTzwKD%7E7bSUFHOaAH3R%7E%7EL0hX6ig%7EYdwA7ycujiPlRrMQ91n1MkwOBbO91IYkBAZqvi2Y-m4diOyAbA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993ddb8e9dc94f5a8ecb03bbb4349cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  98%|#########7| 870M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbe4d4f6adc4ad18725d96a543304be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f17e9714614a21a96934b75eec94f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6de118435fe4ccca25391d357447632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Il est facile de traduire des langues à l'aide de transformateurs\"}]\n"
     ]
    }
   ],
   "source": [
    "# Install tf-keras package\n",
    "%pip install tf-keras\n",
    "\n",
    "traslator = pipeline(\"translation_en_to_fr\")\n",
    "print(traslator(\"It is easy to translate languages with transformers\", \n",
    "                max_length=40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
